{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, accuracy_score, precision_score, f1_score,\n",
    "    confusion_matrix, ConfusionMatrixDisplay, roc_curve\n",
    ")\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "from collections import Counter\n",
    "import warnings\n",
    "import logging\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s [%(levelname)s] %(message)s',\n",
    "    handlers=[\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "try:\n",
    "    train_data = pd.read_csv('train.csv')\n",
    "    test_data = pd.read_csv('test.csv')\n",
    "    logger.info(\"Datasets loaded successfully.\")\n",
    "except FileNotFoundError as e:\n",
    "    logger.error(\"Error loading datasets: \", exc_info=True)\n",
    "    raise e\n",
    "\n",
    "X = train_data.drop(columns=['id', 'smoking'])\n",
    "y = train_data['smoking']\n",
    "logger.info(\"Features and target variable defined.\")\n",
    "\n",
    "numerical_cols = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "categorical_cols = X.select_dtypes(include=['object']).columns.tolist()\n",
    "logger.info(f\"Numerical columns: {numerical_cols}\")\n",
    "logger.info(f\"Categorical columns: {categorical_cols}\")\n",
    "\n",
    "\n",
    "\n",
    "numerical_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('num', numerical_pipeline, numerical_cols),\n",
    "    ('cat', categorical_pipeline, categorical_cols)\n",
    "])\n",
    "logger.info(\"Preprocessing pipelines created.\")\n",
    "\n",
    "counter = Counter(y)\n",
    "scale_pos_weight = counter[0] / counter[1]\n",
    "logger.info(f\"Class distribution: {counter}\")\n",
    "logger.info(f\"Scale_pos_weight: {scale_pos_weight:.2f}\")\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=101, stratify=y\n",
    ")\n",
    "logger.info(\"Data split into training and validation sets.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def objective(trial):\n",
    "    try:\n",
    "        param = {\n",
    "            'classifier__n_estimators': trial.suggest_int('n_estimators', 100, 1000),\n",
    "            'classifier__max_depth': trial.suggest_int('max_depth', 5, 50),\n",
    "            'classifier__min_samples_split': trial.suggest_int('min_samples_split', 2, 20),\n",
    "            'classifier__min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 20),\n",
    "            'classifier__max_features': trial.suggest_categorical('max_features', ['sqrt', 'log2', None]),\n",
    "            'classifier__bootstrap': trial.suggest_categorical('bootstrap', [True, False]),\n",
    "            'classifier__criterion': trial.suggest_categorical('criterion', ['gini', 'entropy'])\n",
    "        }\n",
    "        \n",
    "        pipeline = Pipeline([\n",
    "            ('preprocessor', preprocessor),\n",
    "            ('classifier', RandomForestClassifier(\n",
    "                random_state=101,\n",
    "                class_weight='balanced',\n",
    "                n_jobs=-1\n",
    "            ))\n",
    "        ])\n",
    "        \n",
    "        pipeline.set_params(**param)\n",
    "        \n",
    "        skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=101)\n",
    "        \n",
    "        roc_auc_scores = []\n",
    "        \n",
    "        for train_idx, test_idx in skf.split(X_train, y_train):\n",
    "            X_tr, X_te = X_train.iloc[train_idx], X_train.iloc[test_idx]\n",
    "            y_tr, y_te = y_train.iloc[train_idx], y_train.iloc[test_idx]\n",
    "            \n",
    "            pipeline.fit(X_tr, y_tr)\n",
    "            \n",
    "            y_pred_prob = pipeline.predict_proba(X_te)[:, 1]\n",
    "            \n",
    "            score = roc_auc_score(y_te, y_pred_prob)\n",
    "            roc_auc_scores.append(score)\n",
    "        \n",
    "        return np.mean(roc_auc_scores)\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"An error occurred during trial: {e}\", exc_info=True)\n",
    "        return float('nan')  # Optuna treats NaN as a failed trial\n",
    "\n",
    "sampler = TPESampler(seed=101)\n",
    "study = optuna.create_study(direction='maximize', sampler=sampler)\n",
    "logger.info(\"Starting hyperparameter optimization with Optuna...\")\n",
    "study.optimize(objective, n_trials=100, timeout=3600)  # Adjust n_trials and timeout as needed\n",
    "\n",
    "logger.info(f\"Number of finished trials: {len(study.trials)}\")\n",
    "logger.info(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "\n",
    "logger.info(f\"  Value (Average ROC-AUC): {trial.value:.4f}\")\n",
    "logger.info(\"  Params:\")\n",
    "for key, value in trial.params.items():\n",
    "    logger.info(f\"    {key}: {value}\")\n",
    "\n",
    "best_params = trial.params\n",
    "\n",
    "if not all(key.startswith('classifier__') for key in best_params.keys()):\n",
    "    best_params_prefixed = {f\"classifier__{key}\": value for key, value in best_params.items()}\n",
    "else:\n",
    "    best_params_prefixed = best_params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "final_pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', RandomForestClassifier(\n",
    "        random_state=101,\n",
    "        class_weight='balanced',\n",
    "        n_jobs=-1\n",
    "    ))\n",
    "])\n",
    "\n",
    "final_pipeline.set_params(**best_params_prefixed)\n",
    "logger.info(\"Best hyperparameters set on the final pipeline.\")\n",
    "\n",
    "final_pipeline.fit(X_train, y_train)\n",
    "logger.info(\"Final model trained on the training set.\")\n",
    "\n",
    "y_val_pred = final_pipeline.predict(X_val)\n",
    "y_val_pred_prob = final_pipeline.predict_proba(X_val)[:, 1]\n",
    "logger.info(\"Predictions made on the validation set.\")\n",
    "\n",
    "roc_auc = roc_auc_score(y_val, y_val_pred_prob)\n",
    "accuracy = accuracy_score(y_val, y_val_pred)\n",
    "precision = precision_score(y_val, y_val_pred)\n",
    "f1 = f1_score(y_val, y_val_pred)\n",
    "\n",
    "logger.info(f\"Validation ROC-AUC: {roc_auc:.3f}\")\n",
    "logger.info(f\"Validation Accuracy: {accuracy:.3f}\")\n",
    "logger.info(f\"Validation Precision: {precision:.3f}\")\n",
    "logger.info(f\"Validation F1-Score: {f1:.3f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = roc_curve(y_val, y_val_pred_prob)\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, label=f'ROC curve (area = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], 'k--', label='Random Guess')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "cm = confusion_matrix(y_val, y_val_pred)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "disp.plot(cmap=plt.cm.Blues)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "def get_feature_names(preprocessor):\n",
    "    num_features = numerical_cols\n",
    "    \n",
    "    cat_pipeline = preprocessor.named_transformers_['cat']\n",
    "    onehot = cat_pipeline.named_steps['onehot']\n",
    "    cat_features = onehot.get_feature_names_out(categorical_cols)\n",
    "    \n",
    "    return np.concatenate([num_features, cat_features])\n",
    "\n",
    "try:\n",
    "    feature_names = get_feature_names(preprocessor)\n",
    "    importances = final_pipeline.named_steps['classifier'].feature_importances_\n",
    "    feature_importances_df = pd.DataFrame({\n",
    "        'Feature': feature_names,\n",
    "        'Importance': importances\n",
    "    }).sort_values(by='Importance', ascending=False)\n",
    "    \n",
    "    logger.info(\"Top 20 Feature Importances:\")\n",
    "    print(feature_importances_df.head(20))\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.barplot(x='Importance', y='Feature', data=feature_importances_df.head(20))\n",
    "    plt.title('Top 20 Feature Importances')\n",
    "    plt.xlabel('Importance')\n",
    "    plt.ylabel('Feature')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "except AttributeError as e:\n",
    "    logger.error(\"Error in extracting feature names: \", exc_info=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "final_pipeline.fit(X, y)\n",
    "logger.info(\"Final model retrained on the entire training dataset.\")\n",
    "\n",
    "X_test = test_data.drop(columns=['id'])\n",
    "logger.info(\"Test dataset prepared.\")\n",
    "\n",
    "y_test_pred_prob = final_pipeline.predict_proba(X_test)[:, 1]\n",
    "logger.info(\"Predictions made on the test set.\")\n",
    "\n",
    "# def custom_round(prob):\n",
    "#     if prob > 0.85:\n",
    "#         return 1.0\n",
    "#     elif 0.35 < prob < 0.5:\n",
    "#         return 0.5\n",
    "#     else:\n",
    "#         return round(prob, 2)\n",
    "\n",
    "y_test_pred_prob_rounded = pd.Series(y_test_pred_prob)\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "    'id': test_data['id'],\n",
    "    'smoking': y_test_pred_prob_rounded\n",
    "})\n",
    "\n",
    "# assert submission.isnull().sum().sum() == 0, \"Submission contains missing values.\"\n",
    "# assert set(submission.columns) == {'id', 'smoking'}, \"Submission columns mismatch.\"\n",
    "\n",
    "submission.to_csv('submission_smoking_random_forest_optuna11.csv', index=False)\n",
    "logger.info(\"Submission file saved as 'submission_smoking_random_forest_optuna11.csv'\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
